{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Heat Attack Dataset\r\n",
        "This dataset was downloaded from https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset.<br>\r\n",
        "The notebook was created taking as baseline the tutorials from https://microsoftlearning.github.io/mslearn-dp100/.\r\n",
        "## Step 1: Connect to a workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Dataset\r\n",
        "ws = Workspace.from_config()\r\n",
        "print(ws.name, \"loaded\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635994960507
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check available compute resources. Mostly use CI (local compute) for this stage."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Compute Resources:\")\r\n",
        "for compute_name in ws.compute_targets:\r\n",
        "    compute = ws.compute_targets[compute_name]\r\n",
        "    print(\"\\t\", compute.name, ':', compute.type)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635994961405
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load and register dataset\r\n",
        "**Data Description**<br>\r\n",
        "\r\n",
        "*age*: Age of the person<br>\r\n",
        "*sex*: Gender of the person<br>\r\n",
        "*cp*: chest pain type<br>\r\n",
        "*trtbps*: resting blood pressure (mm Hg)<br>\r\n",
        "*chol*: cholesterol (mg/dl)<br>\r\n",
        "*fbs*: fasting blood sugar > 120 mg/dl<br>\r\n",
        "*restecg*: resting electrocardiographic results<br>\r\n",
        "*thalachh*: maximum heart rate achieved<br>\r\n",
        "*exng*: exercise induced angina (1 = yes, 0 = no)<br>\r\n",
        "*oldpeak*: previous peak<br>\r\n",
        "*slp*: slope<br>\r\n",
        "*caa*: number of major vessels (0-3)<br>\r\n",
        "*thall*: Thal rate <br>\r\n",
        "*output*: had heart attack (target)\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load default datastore\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "# Upload datasets to the datastore\r\n",
        "default_ds.upload_files(['./data/heart.csv', './data/o2Saturation.csv'],\r\n",
        "                        target_path='heart-data/',\r\n",
        "                        overwrite=True,\r\n",
        "                        show_progress=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Uploading an estimated of 2 files\nUploading ./data/heart.csv\nUploaded ./data/heart.csv, 1 files out of an estimated total of 2\nUploading ./data/o2Saturation.csv\nUploaded ./data/o2Saturation.csv, 2 files out of an estimated total of 2\nUploaded 2 files\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "$AZUREML_DATAREFERENCE_70b8907bdabe4730baf582bf9c872eeb"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635994962247
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tabular dataset with heart data\r\n",
        "heart_tab = Dataset.Tabular.from_delimited_files(path=(default_ds, 'heart-data/heart.csv'))\r\n",
        "heart_tab.to_pandas_dataframe()\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "     age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n0     63    1   3     145   233    1        0       150     0      2.3    0   \n1     37    1   2     130   250    0        1       187     0      3.5    0   \n2     41    0   1     130   204    0        0       172     0      1.4    2   \n3     56    1   1     120   236    0        1       178     0      0.8    2   \n4     57    0   0     120   354    0        1       163     1      0.6    2   \n..   ...  ...  ..     ...   ...  ...      ...       ...   ...      ...  ...   \n298   57    0   0     140   241    0        1       123     1      0.2    1   \n299   45    1   3     110   264    0        1       132     0      1.2    1   \n300   68    1   0     144   193    1        1       141     0      3.4    1   \n301   57    1   0     130   131    0        1       115     1      1.2    1   \n302   57    0   1     130   236    0        0       174     0      0.0    1   \n\n     caa  thall  output  \n0      0      1       1  \n1      0      2       1  \n2      0      2       1  \n3      0      2       1  \n4      0      2       1  \n..   ...    ...     ...  \n298    0      3       0  \n299    0      3       0  \n300    2      3       0  \n301    1      3       0  \n302    1      2       0  \n\n[303 rows x 14 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trtbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalachh</th>\n      <th>exng</th>\n      <th>oldpeak</th>\n      <th>slp</th>\n      <th>caa</th>\n      <th>thall</th>\n      <th>output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63</td>\n      <td>1</td>\n      <td>3</td>\n      <td>145</td>\n      <td>233</td>\n      <td>1</td>\n      <td>0</td>\n      <td>150</td>\n      <td>0</td>\n      <td>2.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>37</td>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>250</td>\n      <td>0</td>\n      <td>1</td>\n      <td>187</td>\n      <td>0</td>\n      <td>3.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41</td>\n      <td>0</td>\n      <td>1</td>\n      <td>130</td>\n      <td>204</td>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>1.4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>1</td>\n      <td>1</td>\n      <td>120</td>\n      <td>236</td>\n      <td>0</td>\n      <td>1</td>\n      <td>178</td>\n      <td>0</td>\n      <td>0.8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>120</td>\n      <td>354</td>\n      <td>0</td>\n      <td>1</td>\n      <td>163</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>140</td>\n      <td>241</td>\n      <td>0</td>\n      <td>1</td>\n      <td>123</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>45</td>\n      <td>1</td>\n      <td>3</td>\n      <td>110</td>\n      <td>264</td>\n      <td>0</td>\n      <td>1</td>\n      <td>132</td>\n      <td>0</td>\n      <td>1.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>68</td>\n      <td>1</td>\n      <td>0</td>\n      <td>144</td>\n      <td>193</td>\n      <td>1</td>\n      <td>1</td>\n      <td>141</td>\n      <td>0</td>\n      <td>3.4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>301</th>\n      <td>57</td>\n      <td>1</td>\n      <td>0</td>\n      <td>130</td>\n      <td>131</td>\n      <td>0</td>\n      <td>1</td>\n      <td>115</td>\n      <td>1</td>\n      <td>1.2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>302</th>\n      <td>57</td>\n      <td>0</td>\n      <td>1</td>\n      <td>130</td>\n      <td>236</td>\n      <td>0</td>\n      <td>0</td>\n      <td>174</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>303 rows × 14 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635994977422
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tabular dataset with O2 saturation data\r\n",
        "o2_tab = Dataset.Tabular.from_delimited_files(path=(default_ds, 'heart-data/o2Saturation.csv'))\r\n",
        "o2_tab.to_pandas_dataframe()\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "      98.6\n0     98.6\n1     98.6\n2     98.6\n3     98.1\n4     97.5\n...    ...\n3580  98.6\n3581  98.6\n3582  98.6\n3583  98.6\n3584  98.6\n\n[3585 rows x 1 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>98.6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>98.6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>98.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>98.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>98.1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>97.5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3580</th>\n      <td>98.6</td>\n    </tr>\n    <tr>\n      <th>3581</th>\n      <td>98.6</td>\n    </tr>\n    <tr>\n      <th>3582</th>\n      <td>98.6</td>\n    </tr>\n    <tr>\n      <th>3583</th>\n      <td>98.6</td>\n    </tr>\n    <tr>\n      <th>3584</th>\n      <td>98.6</td>\n    </tr>\n  </tbody>\n</table>\n<p>3585 rows × 1 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635994984573
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register heart dataset\r\n",
        "heart_tab = heart_tab.register(workspace=ws,\r\n",
        "                            name='heart',\r\n",
        "                            description='heart attack data',\r\n",
        "                            tags={'format':'CSV'},\r\n",
        "                            create_new_version=True)\r\n",
        "\r\n",
        "# Register o2 dataset\r\n",
        "o2_tab = o2_tab.register(workspace=ws,\r\n",
        "                            name='o2',\r\n",
        "                            description='o2 saturation data',\r\n",
        "                            tags={'format':'CSV'},\r\n",
        "                            create_new_version=True)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635994985196
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Datasets:\")\r\n",
        "for dataset_name in list(ws.datasets.keys()):\r\n",
        "    dataset = Dataset.get_by_name(ws, dataset_name)\r\n",
        "    print(\"\\t\", dataset.name, 'version', dataset.version)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Datasets:\n\t o2 version 1\n\t heart version 2\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635994985580
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Check data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for Null values\r\n",
        "heart_tab.to_pandas_dataframe().isnull().sum()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "age         0\nsex         0\ncp          0\ntrtbps      0\nchol        0\nfbs         0\nrestecg     0\nthalachh    0\nexng        0\noldpeak     0\nslp         0\ncaa         0\nthall       0\noutput      0\ndtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635994987097
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look inside\r\n",
        "heart_tab.to_pandas_dataframe().describe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "              age         sex          cp      trtbps        chol         fbs  \\\ncount  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \nmean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \nstd      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \nmin     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \nmax     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n\n          restecg    thalachh        exng     oldpeak         slp         caa  \\\ncount  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \nmean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \nstd      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \nmin      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \nmax      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n\n            thall      output  \ncount  303.000000  303.000000  \nmean     2.313531    0.544554  \nstd      0.612277    0.498835  \nmin      0.000000    0.000000  \n25%      2.000000    0.000000  \n50%      2.000000    1.000000  \n75%      3.000000    1.000000  \nmax      3.000000    1.000000  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trtbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalachh</th>\n      <th>exng</th>\n      <th>oldpeak</th>\n      <th>slp</th>\n      <th>caa</th>\n      <th>thall</th>\n      <th>output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>54.366337</td>\n      <td>0.683168</td>\n      <td>0.966997</td>\n      <td>131.623762</td>\n      <td>246.264026</td>\n      <td>0.148515</td>\n      <td>0.528053</td>\n      <td>149.646865</td>\n      <td>0.326733</td>\n      <td>1.039604</td>\n      <td>1.399340</td>\n      <td>0.729373</td>\n      <td>2.313531</td>\n      <td>0.544554</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9.082101</td>\n      <td>0.466011</td>\n      <td>1.032052</td>\n      <td>17.538143</td>\n      <td>51.830751</td>\n      <td>0.356198</td>\n      <td>0.525860</td>\n      <td>22.905161</td>\n      <td>0.469794</td>\n      <td>1.161075</td>\n      <td>0.616226</td>\n      <td>1.022606</td>\n      <td>0.612277</td>\n      <td>0.498835</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>29.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>94.000000</td>\n      <td>126.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>71.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>47.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>120.000000</td>\n      <td>211.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>133.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>55.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>130.000000</td>\n      <td>240.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>153.000000</td>\n      <td>0.000000</td>\n      <td>0.800000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>61.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>140.000000</td>\n      <td>274.500000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>166.000000</td>\n      <td>1.000000</td>\n      <td>1.600000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>77.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>200.000000</td>\n      <td>564.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>202.000000</td>\n      <td>1.000000</td>\n      <td>6.200000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635994987326
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Train a model from script\r\n",
        "\r\n",
        "Create an experiment folder."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "# Create a folder for the experiment files\r\n",
        "experiment_folder = 'heart_training_1'\r\n",
        "os.makedirs(experiment_folder, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635994987661
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an environment file."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/environment.yml\r\n",
        "name: experiment_env\r\n",
        "dependencies:\r\n",
        "- python=3.6.2\r\n",
        "- scikit-learn\r\n",
        "- ipykernel\r\n",
        "- matplotlib\r\n",
        "- pandas\r\n",
        "- pip\r\n",
        "- pip:\r\n",
        "  - azureml-defaults\r\n",
        "  - pyarrow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting heart_training_1/environment.yml\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating experiment script, using a random forest classifier."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/heart_training.py\r\n",
        "# Import libraries\r\n",
        "import os\r\n",
        "import argparse\r\n",
        "from azureml.core import Run, Dataset\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import joblib\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "\r\n",
        "# Get script arguments (mostly the training dataset ID at this point)\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--ds\", type=str, dest='ds_id')\r\n",
        "args = parser.parse_args()\r\n",
        "\r\n",
        "# Get experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "# Get training dataset\r\n",
        "print(\"Loading Data...\")\r\n",
        "ws = run.experiment.workspace\r\n",
        "heart = run.input_datasets['heart_dataset'].to_pandas_dataframe()\r\n",
        "\r\n",
        "# Separate features and labels\r\n",
        "y = heart['output'].values\r\n",
        "X = heart.drop(['output'], axis=1).values\r\n",
        "print(X)\r\n",
        "\r\n",
        "# Split data into train and test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\r\n",
        "\r\n",
        "# Train a random forest model\r\n",
        "print('Training a Random Forest Classifier model with default hyperparameters.')\r\n",
        "model = RandomForestClassifier().fit(X_train, y_train)\r\n",
        "\r\n",
        "# Calculate accuracy\r\n",
        "y_hat = model.predict(X_test)\r\n",
        "model_accuracy = np.average(y_hat == y_test)\r\n",
        "print('Accuracy: ', model_accuracy)\r\n",
        "run.log('Accuracy', np.float(model_accuracy))\r\n",
        "\r\n",
        "# Calculate AUC\r\n",
        "y_scores = model.predict_proba(X_test)\r\n",
        "auc = roc_auc_score(y_test, y_scores[:,1])\r\n",
        "print('AUC: ', str(auc))\r\n",
        "run.log('AUC', np.float(auc))\r\n",
        "\r\n",
        "os.makedirs('outputs', exist_ok=True)\r\n",
        "joblib.dump(value=model, filename='outputs/heart_model1.pkl')\r\n",
        "\r\n",
        "run.complete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting heart_training_1/heart_training.py\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the experiment script."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "# Create python environment for the experiment (from a .yml file)\r\n",
        "env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/environment.yml\")\r\n",
        "\r\n",
        "# Get training dataset\r\n",
        "heart_ds = ws.datasets.get(\"heart\")\r\n",
        "\r\n",
        "# Get a script config\r\n",
        "script_config = ScriptRunConfig(source_directory=experiment_folder,\r\n",
        "                                script='heart_training.py',\r\n",
        "                                arguments=['--ds', heart_ds.as_named_input('heart_dataset')],\r\n",
        "                                environment=env)\r\n",
        "\r\n",
        "# Submit the experiment\r\n",
        "experiment_name = 'train-heart'\r\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\r\n",
        "run = experiment.submit(config=script_config)\r\n",
        "RunDetails(run).show()\r\n",
        "run.wait_for_completion()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c121a53dd8e4f2fb3df909e84c3ccdb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/train-heart_1635994987_e1effee5?wsid=/subscriptions/d4611b57-d550-4cdc-90b8-97e2b8bf45d1/resourcegroups/rg-ai-915-002/workspaces/wsp-ai-915-002&tid=c609a0ec-a5e3-4631-9686-192280bd9151\", \"run_id\": \"train-heart_1635994987_e1effee5\", \"run_properties\": {\"run_id\": \"train-heart_1635994987_e1effee5\", \"created_utc\": \"2021-11-04T03:03:08.520529Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": \"eb7801e7-c618-47ed-aee2-dfd11025b580\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2021-11-04T03:03:40.725138Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/60_control_log.txt\": \"https://wspai9150022834122947.blob.core.windows.net/azureml/ExperimentRun/dcid.train-heart_1635994987_e1effee5/azureml-logs/60_control_log.txt?sv=2019-07-07&sr=b&sig=4%2Bc4RY4nKxqgtyJ%2FE3enwJqS7u3sfy%2FWhdQ9mdBSdNs%3D&skoid=e5e6cfcb-9f6d-4652-b9cf-6f37b8f74f99&sktid=c609a0ec-a5e3-4631-9686-192280bd9151&skt=2021-11-04T02%3A53%3A10Z&ske=2021-11-05T11%3A03%3A10Z&sks=b&skv=2019-07-07&st=2021-11-04T02%3A58%3A46Z&se=2021-11-04T11%3A08%3A46Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://wspai9150022834122947.blob.core.windows.net/azureml/ExperimentRun/dcid.train-heart_1635994987_e1effee5/azureml-logs/70_driver_log.txt?sv=2019-07-07&sr=b&sig=JVOw8CmfBJDiN5LHg8IDTDkYq6ffaNGVFEwEwTqAz08%3D&skoid=e5e6cfcb-9f6d-4652-b9cf-6f37b8f74f99&sktid=c609a0ec-a5e3-4631-9686-192280bd9151&skt=2021-11-04T02%3A53%3A10Z&ske=2021-11-05T11%3A03%3A10Z&sks=b&skv=2019-07-07&st=2021-11-04T02%3A58%3A46Z&se=2021-11-04T11%3A08%3A46Z&sp=r\", \"logs/azureml/3839_azureml.log\": \"https://wspai9150022834122947.blob.core.windows.net/azureml/ExperimentRun/dcid.train-heart_1635994987_e1effee5/logs/azureml/3839_azureml.log?sv=2019-07-07&sr=b&sig=o%2BuRmc%2BPlGcPg58xLikJnv2NIlpZkr2En0K43a6bTVE%3D&skoid=e5e6cfcb-9f6d-4652-b9cf-6f37b8f74f99&sktid=c609a0ec-a5e3-4631-9686-192280bd9151&skt=2021-11-04T02%3A53%3A10Z&ske=2021-11-05T11%3A03%3A10Z&sks=b&skv=2019-07-07&st=2021-11-04T02%3A58%3A35Z&se=2021-11-04T11%3A08%3A35Z&sp=r\", \"logs/azureml/dataprep/backgroundProcess.log\": \"https://wspai9150022834122947.blob.core.windows.net/azureml/ExperimentRun/dcid.train-heart_1635994987_e1effee5/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=fr0u7YhUT8j0%2Fqo7Vaz3qH%2B6soqav9Psto7v7377z8o%3D&skoid=e5e6cfcb-9f6d-4652-b9cf-6f37b8f74f99&sktid=c609a0ec-a5e3-4631-9686-192280bd9151&skt=2021-11-04T02%3A53%3A10Z&ske=2021-11-05T11%3A03%3A10Z&sks=b&skv=2019-07-07&st=2021-11-04T02%3A58%3A35Z&se=2021-11-04T11%3A08%3A35Z&sp=r\", \"logs/azureml/dataprep/backgroundProcess_Telemetry.log\": \"https://wspai9150022834122947.blob.core.windows.net/azureml/ExperimentRun/dcid.train-heart_1635994987_e1effee5/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=IYz8ExNLDvsqthKCKT30mA3TIoMErQHaI2LpcwLyem0%3D&skoid=e5e6cfcb-9f6d-4652-b9cf-6f37b8f74f99&sktid=c609a0ec-a5e3-4631-9686-192280bd9151&skt=2021-11-04T02%3A53%3A10Z&ske=2021-11-05T11%3A03%3A10Z&sks=b&skv=2019-07-07&st=2021-11-04T02%3A58%3A35Z&se=2021-11-04T11%3A08%3A35Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/dataprep/backgroundProcess.log\", \"logs/azureml/dataprep/backgroundProcess_Telemetry.log\"], [\"azureml-logs/60_control_log.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"logs/azureml/3839_azureml.log\"]], \"run_duration\": \"0:00:32\", \"run_number\": \"13\", \"run_queued_details\": {\"status\": \"Completed\", \"details\": null}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"Accuracy\", \"run_id\": \"train-heart_1635994987_e1effee5\", \"categories\": [0], \"series\": [{\"data\": [0.8289473684210527]}]}, {\"name\": \"AUC\", \"run_id\": \"train-heart_1635994987_e1effee5\", \"categories\": [0], \"series\": [{\"data\": [0.9270613107822411]}]}], \"run_logs\": \"2021-11-04 03:03:13,931|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'EnableMLflowTracking': True, 'snapshotProject': True}, track_folders: None, deny_list: None, directories_to_watch: ['logs', 'logs/azureml']\\n2021-11-04 03:03:13,939|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: none\\n2021-11-04 03:03:13,940|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Failed to import pyspark with error: No module named 'pyspark'\\n2021-11-04 03:03:13,940|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2021-11-04 03:03:15,546|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7f5dd3c7ae18> for run source azureml.scriptrun\\n2021-11-04 03:03:15,547|azureml.core.run|DEBUG|Identity in use is not set. Falling back to using AMLToken\\n2021-11-04 03:03:15,547|azureml.core.run|DEBUG|Using AMLToken auth for remote run\\n2021-11-04 03:03:15,555|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2021-11-04 03:03:15,564|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2021-11-04 03:03:15,564|azureml.core.authentication|DEBUG|Time to expire 1814392.435039 seconds\\n2021-11-04 03:03:15,565|azureml._restclient.service_context|DEBUG|Created a static thread pool for ServiceContext class\\n2021-11-04 03:03:15,565|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:15,565|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:15,566|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:15,566|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:15,566|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:15,566|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:15,566|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:16,608|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2021-11-04 03:03:16,608|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2021-11-04 03:03:16,691|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2021-11-04 03:03:16,692|azureml._SubmittedRun#train-heart_1635994987_e1effee5|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': 'eb7801e7-c618-47ed-aee2-dfd11025b580'}\\n2021-11-04 03:03:16,692|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2021-11-04 03:03:16,693|azureml|WARNING|Could not import azureml.mlflow or azureml.contrib.mlflow mlflow APIs will not run against AzureML services.  Add azureml-mlflow as a conda dependency for the run if this behavior is desired\\n2021-11-04 03:03:16,693|azureml.WorkerPool|DEBUG|[START]\\n2021-11-04 03:03:16,693|azureml.SendRunKillSignal|DEBUG|[START]\\n2021-11-04 03:03:16,693|azureml.RunStatusContext|DEBUG|[START]\\n2021-11-04 03:03:16,693|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunContextManager.RunStatusContext|DEBUG|[START]\\n2021-11-04 03:03:16,693|azureml.MetricsClient|DEBUG|[START]\\n2021-11-04 03:03:16,693|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient|DEBUG|[START]\\n2021-11-04 03:03:16,693|azureml.ContentUploader|DEBUG|[START]\\n2021-11-04 03:03:16,694|azureml._history.utils.context_managers|DEBUG|starting file watcher\\n2021-11-04 03:03:16,694|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|[Start]\\n2021-11-04 03:03:16,694|azureml.TrackFolders|DEBUG|[START]\\n2021-11-04 03:03:16,695|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2021-11-04 03:03:16,695|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2021-11-04 03:03:16,695|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /tmp/azureml_runs/train-heart_1635994987_e1effee5\\n2021-11-04 03:03:16,695|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2021-11-04 03:03:16,695|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /tmp/azureml_runs/train-heart_1635994987_e1effee5\\n2021-11-04 03:03:16,710|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2021-11-04 03:03:16,710|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2021-11-04 03:03:16,873|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2021-11-04 03:03:16,970|azureml._history.utils.context_managers.FileWatcher|DEBUG|uploading data to container: azureml blob: ExperimentRun/dcid.train-heart_1635994987_e1effee5/logs/azureml/3839_azureml.log path: /tmp/azureml_runs/train-heart_1635994987_e1effee5/logs/azureml/3839_azureml.log\\n2021-11-04 03:03:16,971|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2021-11-04 03:03:16,971|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result|DEBUG|Using basic handler - no exception handling\\n2021-11-04 03:03:16,975|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 0_result to queue of approximate size: 0\\n2021-11-04 03:03:21,030|azureml.core.run|DEBUG|Identity in use is not set. Falling back to using AMLToken\\n2021-11-04 03:03:21,030|azureml.core.run|DEBUG|Using AMLToken auth for remote run\\n2021-11-04 03:03:21,030|azureml._restclient.service_context|DEBUG|Access an existing static threadpool for ServiceContext class\\n2021-11-04 03:03:21,030|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,031|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,031|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,031|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,031|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,031|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,031|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,068|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2021-11-04 03:03:21,068|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2021-11-04 03:03:21,132|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2021-11-04 03:03:21,133|azureml._SubmittedRun#train-heart_1635994987_e1effee5|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': 'eb7801e7-c618-47ed-aee2-dfd11025b580'}\\n2021-11-04 03:03:21,133|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2021-11-04 03:03:21,347|azureml.core.run|DEBUG|Identity in use is not set. Falling back to using AMLToken\\n2021-11-04 03:03:21,347|azureml.core.run|DEBUG|Using AMLToken auth for remote run\\n2021-11-04 03:03:21,347|azureml._restclient.service_context|DEBUG|Access an existing static threadpool for ServiceContext class\\n2021-11-04 03:03:21,349|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,349|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,350|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,350|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,350|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,350|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:21,350|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://eastus2.api.azureml.ms.\\n2021-11-04 03:03:26,977|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2021-11-04 03:03:26,977|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2021-11-04 03:03:27,165|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2021-11-04 03:03:27,233|azureml._history.utils.context_managers.FileWatcher|DEBUG|uploading data to container: azureml blob: ExperimentRun/dcid.train-heart_1635994987_e1effee5/logs/azureml/dataprep/backgroundProcess_Telemetry.log path: /tmp/azureml_runs/train-heart_1635994987_e1effee5/logs/azureml/dataprep/backgroundProcess_Telemetry.log\\n2021-11-04 03:03:27,273|azureml._history.utils.context_managers.FileWatcher|DEBUG|uploading data to container: azureml blob: ExperimentRun/dcid.train-heart_1635994987_e1effee5/logs/azureml/dataprep/backgroundProcess.log path: /tmp/azureml_runs/train-heart_1635994987_e1effee5/logs/azureml/dataprep/backgroundProcess.log\\n2021-11-04 03:03:27,273|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2021-11-04 03:03:27,274|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result|DEBUG|Using basic handler - no exception handling\\n2021-11-04 03:03:27,274|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 1_result to queue of approximate size: 1\\n2021-11-04 03:03:27,278|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2021-11-04 03:03:27,288|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result|DEBUG|Using basic handler - no exception handling\\n2021-11-04 03:03:27,288|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 2_result to queue of approximate size: 2\\n2021-11-04 03:03:27,288|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2021-11-04 03:03:27,294|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result|DEBUG|Using basic handler - no exception handling\\n2021-11-04 03:03:27,296|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 3_result to queue of approximate size: 3\\n2021-11-04 03:03:27,590|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2021-11-04 03:03:27,591|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.PostMetricsBatchV2Daemon|DEBUG|Starting daemon and triggering first instance\\n2021-11-04 03:03:27,591|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2021-11-04 03:03:27,667|azureml._SubmittedRun#train-heart_1635994987_e1effee5|INFO|complete is not setting status for submitted runs.\\n2021-11-04 03:03:27,668|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2021-11-04 03:03:27,668|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2021-11-04 03:03:27,668|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2021-11-04 03:03:27,668|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2021-11-04 03:03:27,668|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2021-11-04 03:03:27,668|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2021-11-04 03:03:27,668|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [].\\n2021-11-04 03:03:27,668|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2021-11-04 03:03:27,669|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2021-11-04 03:03:27,669|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Start]\\n2021-11-04 03:03:27,669|azureml.BatchTaskQueueAdd_1_Batches.WorkerPool|DEBUG|submitting future: _handle_batch\\n2021-11-04 03:03:27,669|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|Batch size 2.\\n2021-11-04 03:03:27,669|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch|DEBUG|Using basic handler - no exception handling\\n2021-11-04 03:03:27,669|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|Adding task 0__handle_batch to queue of approximate size: 0\\n2021-11-04 03:03:27,669|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Stop] - waiting default timeout\\n2021-11-04 03:03:27,669|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[START]\\n2021-11-04 03:03:27,669|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Overriding default flush timeout from None to 120\\n2021-11-04 03:03:27,669|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Waiting 120 seconds on tasks: [AsyncTask(0__handle_batch)].\\n2021-11-04 03:03:27,669|azureml._restclient.service_context.WorkerPool|DEBUG|submitting future: _log_batch_v2\\n2021-11-04 03:03:27,672|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient|DEBUG|Metrics Client: _log_batch_v2 is calling post_run_metrics posting 2 values.\\n2021-11-04 03:03:27,672|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.0__log_batch_v2|DEBUG|Using basic handler - no exception handling\\n2021-11-04 03:03:27,672|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient._post_run_metrics_log_failed_validations-async:False|DEBUG|[START]\\n2021-11-04 03:03:27,672|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|Adding task 0__log_batch_v2 to queue of approximate size: 0\\n2021-11-04 03:03:27,672|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient|DEBUG|ClientBase: Calling _post_run_metrics_log_failed_validations with url None\\n2021-11-04 03:03:27,847|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient._post_run_metrics_log_failed_validations-async:False|DEBUG|[STOP]\\n2021-11-04 03:03:27,920|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[START]\\n2021-11-04 03:03:27,920|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|Awaiter is BatchTaskQueueAdd_1_Batches\\n2021-11-04 03:03:27,920|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[STOP]\\n2021-11-04 03:03:27,920|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|Waiting on task: 0__handle_batch.\\n1 tasks left. Current duration of flush 8.893013000488281e-05 seconds.\\n\\n2021-11-04 03:03:27,920|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[STOP]\\n2021-11-04 03:03:27,920|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2021-11-04 03:03:27,920|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2021-11-04 03:03:27,921|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [AsyncTask(0__log_batch_v2)].\\n2021-11-04 03:03:27,921|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.0__log_batch_v2.WaitingTask|DEBUG|[START]\\n2021-11-04 03:03:27,921|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.0__log_batch_v2.WaitingTask|DEBUG|Awaiter is PostMetricsBatchV2\\n2021-11-04 03:03:27,921|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.0__log_batch_v2.WaitingTask|DEBUG|[STOP]\\n2021-11-04 03:03:27,921|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|\\n2021-11-04 03:03:27,921|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2021-11-04 03:03:27,921|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2021-11-04 03:03:27,921|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.wait_on_ingest-async:False|DEBUG|[START]\\n2021-11-04 03:03:27,921|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient|DEBUG|ClientBase: Calling wait_on_ingest with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experiments/{experimentName}/runs/{runId}/metricsingest/wait\\n2021-11-04 03:03:27,964|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.MetricsClient.wait_on_ingest-async:False|DEBUG|[STOP]\\n2021-11-04 03:03:32,970|azureml._restclient.clientbase|DEBUG|ClientBase: Calling update_status with url None\\n2021-11-04 03:03:33,010|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Uploading tracked directories: [], excluding []\\n2021-11-04 03:03:33,010|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling track for pyfs\\n2021-11-04 03:03:33,081|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2021-11-04 03:03:33,081|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /tmp/azureml_runs/train-heart_1635994987_e1effee5\\n2021-11-04 03:03:33,081|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Reverting working dir from /tmp/azureml_runs/train-heart_1635994987_e1effee5 to /tmp/azureml_runs/train-heart_1635994987_e1effee5\\n2021-11-04 03:03:33,081|azureml.history._tracking.PythonWorkingDirectory|INFO|Working dir is already updated /tmp/azureml_runs/train-heart_1635994987_e1effee5\\n2021-11-04 03:03:33,081|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[STOP]\\n2021-11-04 03:03:33,081|azureml.WorkingDirectoryCM|DEBUG|[STOP]\\n2021-11-04 03:03:33,081|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Uploading tracked directories: ['./outputs'], excluding ['azureml-logs/driver_log']\\n2021-11-04 03:03:33,081|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling track for pyfs\\n2021-11-04 03:03:33,081|azureml.history._tracking.PythonWorkingDirectory|DEBUG|./outputs exists as directory, uploading..\\n2021-11-04 03:03:33,081|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Found and adding path to upload: ./outputs/heart_model1.pkl\\n2021-11-04 03:03:33,081|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Paths to upload is ['./outputs/heart_model1.pkl'] in dir ./outputs\\n2021-11-04 03:03:33,081|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Overriding default timeout to 300\\n2021-11-04 03:03:33,081|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|[Start]\\n2021-11-04 03:03:33,082|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2021-11-04 03:03:33,082|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2021-11-04 03:03:33,208|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2021-11-04 03:03:33,208|azureml._restclient.service_context.WorkerPool|DEBUG|submitting future: perform_upload\\n2021-11-04 03:03:33,209|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload|DEBUG|Using basic handler - no exception handling\\n2021-11-04 03:03:33,209|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Adding task 0_perform_upload to queue of approximate size: 0\\n2021-11-04 03:03:33,209|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|[Stop] - waiting default timeout\\n2021-11-04 03:03:33,209|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|[START]\\n2021-11-04 03:03:33,209|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|Overriding default flush timeout from None to 300\\n2021-11-04 03:03:33,209|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|Waiting 300 seconds on tasks: [AsyncTask(0_perform_upload)].\\n2021-11-04 03:03:33,209|azureml._restclient.clientbase|DEBUG|ClientBase: Calling create_blob_from_stream with url None\\n2021-11-04 03:03:33,325|azureml._file_utils.upload|DEBUG|Uploaded blob ExperimentRun/dcid.train-heart_1635994987_e1effee5/outputs/heart_model1.pkl with size 614826, file size 614826.\\n2021-11-04 03:03:33,460|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|[START]\\n2021-11-04 03:03:33,460|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|Awaiter is upload_files\\n2021-11-04 03:03:33,460|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|[STOP]\\n2021-11-04 03:03:33,460|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Waiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 9.560585021972656e-05 seconds.\\n\\n2021-11-04 03:03:33,460|azureml._SubmittedRun#train-heart_1635994987_e1effee5.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|[STOP]\\n2021-11-04 03:03:33,460|azureml.TrackFolders|DEBUG|[STOP]\\n2021-11-04 03:03:33,460|azureml._history.utils.context_managers|DEBUG|exiting ContentUploader, waiting for file_watcher to finish upload...\\n2021-11-04 03:03:33,460|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher called finish, setting event\\n2021-11-04 03:03:33,460|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher received exit event, getting current_stat\\n2021-11-04 03:03:33,461|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2021-11-04 03:03:33,466|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result|DEBUG|Using basic handler - no exception handling\\n2021-11-04 03:03:33,466|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 4_result to queue of approximate size: 4\\n2021-11-04 03:03:33,466|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2021-11-04 03:03:33,467|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result|DEBUG|Using basic handler - no exception handling\\n2021-11-04 03:03:33,467|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 5_result to queue of approximate size: 5\\n2021-11-04 03:03:33,467|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher retrieved current_stat, will upload to current_stat\\n2021-11-04 03:03:33,467|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,472|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,472|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,472|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,473|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,473|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,473|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,473|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,474|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,474|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,474|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,474|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,475|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,475|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,475|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,475|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2021-11-04 03:03:33,477|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2021-11-04 03:03:33,483|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result|DEBUG|Using basic handler - no exception handling\\n2021-11-04 03:03:33,483|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 6_result to queue of approximate size: 6\\n2021-11-04 03:03:33,483|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher finished uploading to current_stat, finishing task queue\\n2021-11-04 03:03:33,483|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|[Stop] - waiting default timeout\\n2021-11-04 03:03:33,483|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|[START]\\n2021-11-04 03:03:33,483|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|Overriding default flush timeout from None to 120\\n2021-11-04 03:03:33,483|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|Waiting 120 seconds on tasks: [AsyncTask(0_result), AsyncTask(1_result), AsyncTask(2_result), AsyncTask(3_result), AsyncTask(4_result), AsyncTask(5_result), AsyncTask(6_result)].\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|[START]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|[STOP]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|[START]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|[STOP]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|[START]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|[STOP]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|[START]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|[STOP]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|[START]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|[STOP]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result.WaitingTask|DEBUG|[START]\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2021-11-04 03:03:33,484|azureml._history.utils.context_managers.FileWatcher.UploadQueue.5_result.WaitingTask|DEBUG|[STOP]\\n2021-11-04 03:03:33,735|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result.WaitingTask|DEBUG|[START]\\n2021-11-04 03:03:33,735|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2021-11-04 03:03:33,735|azureml._history.utils.context_managers.FileWatcher.UploadQueue.6_result.WaitingTask|DEBUG|[STOP]\\n2021-11-04 03:03:33,735|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Waiting on task: 6_result.\\n1 tasks left. Current duration of flush 0.0010378360748291016 seconds.\\n\\n2021-11-04 03:03:33,735|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|[STOP]\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.34.0\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "{'runId': 'train-heart_1635994987_e1effee5',\n 'target': 'local',\n 'status': 'Finalizing',\n 'startTimeUtc': '2021-11-04T03:03:09.418237Z',\n 'services': {},\n 'properties': {'_azureml.ComputeTargetType': 'local',\n  'ContentSnapshotId': 'eb7801e7-c618-47ed-aee2-dfd11025b580'},\n 'inputDatasets': [{'dataset': {'id': 'd27bcc02-cca7-433b-81f5-7bb4c8fac898'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'heart_dataset', 'mechanism': 'Direct'}}],\n 'outputDatasets': [],\n 'runDefinition': {'script': 'heart_training.py',\n  'command': '',\n  'useAbsolutePath': False,\n  'arguments': ['--ds', 'DatasetConsumptionConfig:heart_dataset'],\n  'sourceDirectoryDataStore': None,\n  'framework': 'Python',\n  'communicator': 'None',\n  'target': 'local',\n  'dataReferences': {},\n  'data': {'heart_dataset': {'dataLocation': {'dataset': {'id': 'd27bcc02-cca7-433b-81f5-7bb4c8fac898',\n      'name': 'heart',\n      'version': '2'},\n     'dataPath': None,\n     'uri': None},\n    'mechanism': 'Direct',\n    'environmentVariableName': 'heart_dataset',\n    'pathOnCompute': None,\n    'overwrite': False,\n    'options': None}},\n  'outputData': {},\n  'datacaches': [],\n  'jobName': None,\n  'maxRunDurationSeconds': 2592000,\n  'nodeCount': 1,\n  'instanceTypes': [],\n  'priority': None,\n  'credentialPassthrough': False,\n  'identity': None,\n  'environment': {'name': 'experiment_env',\n   'version': 'Autosave_2021-10-27T04:01:35Z_56ff0c4a',\n   'python': {'interpreterPath': 'python',\n    'userManagedDependencies': False,\n    'condaDependencies': {'dependencies': ['python=3.6.2',\n      'scikit-learn',\n      'ipykernel',\n      'matplotlib',\n      'pandas',\n      'pip',\n      {'pip': ['azureml-defaults', 'pyarrow']}],\n     'name': 'azureml_0c5a9aa2def4b3c2501c1f40287a356b'},\n    'baseCondaEnvironment': None},\n   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210806.v1',\n    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n    'baseDockerfile': None,\n    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n    'enabled': False,\n    'arguments': []},\n   'spark': {'repositories': [], 'packages': [], 'precachePackages': True}},\n  'history': {'outputCollection': True,\n   'directoriesToWatch': ['logs'],\n   'enableMLflowTracking': True,\n   'snapshotProject': True},\n  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n    'spark.yarn.maxAppAttempts': '1'}},\n  'parallelTask': {'maxRetriesPerWorker': 0,\n   'workerCountPerNode': 1,\n   'terminalExitCodes': None,\n   'configuration': {}},\n  'amlCompute': {'name': None,\n   'vmSize': None,\n   'retainCluster': False,\n   'clusterMaxNodeCount': None},\n  'aiSuperComputer': {'instanceType': 'D2',\n   'imageVersion': 'pytorch-1.7.0',\n   'location': None,\n   'aiSuperComputerStorageData': None,\n   'interactive': False,\n   'scalePolicy': None,\n   'virtualClusterArmId': None,\n   'tensorboardLogDirectory': None,\n   'sshPublicKey': None,\n   'sshPublicKeys': None,\n   'enableAzmlInt': True,\n   'priority': 'Medium',\n   'slaTier': 'Standard',\n   'userAlias': None},\n  'kubernetesCompute': {'instanceType': None},\n  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n  'mpi': {'processCountPerNode': 1},\n  'pyTorch': {'communicationBackend': 'nccl', 'processCount': None},\n  'hdi': {'yarnDeployMode': 'Cluster'},\n  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n  'exposedPorts': None,\n  'docker': {'useDocker': False,\n   'sharedVolumes': True,\n   'shmSize': '2g',\n   'arguments': []},\n  'cmk8sCompute': {'configuration': {}},\n  'commandReturnCodeConfig': {'returnCode': 'Zero',\n   'successfulReturnCodes': []},\n  'environmentVariables': {},\n  'applicationEndpoints': {},\n  'parameters': []},\n 'logFiles': {'azureml-logs/60_control_log.txt': 'https://wspai9150022834122947.blob.core.windows.net/azureml/ExperimentRun/dcid.train-heart_1635994987_e1effee5/azureml-logs/60_control_log.txt?sv=2019-07-07&sr=b&sig=MN4XbEsimkni2P9IsByZtrbded5Q4CvXunyWwKRLQYs%3D&skoid=e5e6cfcb-9f6d-4652-b9cf-6f37b8f74f99&sktid=c609a0ec-a5e3-4631-9686-192280bd9151&skt=2021-11-04T02%3A53%3A10Z&ske=2021-11-05T11%3A03%3A10Z&sks=b&skv=2019-07-07&st=2021-11-04T02%3A53%3A34Z&se=2021-11-04T11%3A03%3A34Z&sp=r',\n  'azureml-logs/70_driver_log.txt': 'https://wspai9150022834122947.blob.core.windows.net/azureml/ExperimentRun/dcid.train-heart_1635994987_e1effee5/azureml-logs/70_driver_log.txt?sv=2019-07-07&sr=b&sig=69f4bTWh5yPeLxDsZwDREUMlAJYmkt%2BJhkYhL8JpZuk%3D&skoid=e5e6cfcb-9f6d-4652-b9cf-6f37b8f74f99&sktid=c609a0ec-a5e3-4631-9686-192280bd9151&skt=2021-11-04T02%3A53%3A10Z&ske=2021-11-05T11%3A03%3A10Z&sks=b&skv=2019-07-07&st=2021-11-04T02%3A53%3A34Z&se=2021-11-04T11%3A03%3A34Z&sp=r',\n  'logs/azureml/3839_azureml.log': 'https://wspai9150022834122947.blob.core.windows.net/azureml/ExperimentRun/dcid.train-heart_1635994987_e1effee5/logs/azureml/3839_azureml.log?sv=2019-07-07&sr=b&sig=FzHS74jlsl3n7lCsN8n3jPII9MPmsojnRx31W8zMJe4%3D&skoid=e5e6cfcb-9f6d-4652-b9cf-6f37b8f74f99&sktid=c609a0ec-a5e3-4631-9686-192280bd9151&skt=2021-11-04T02%3A53%3A10Z&ske=2021-11-05T11%3A03%3A10Z&sks=b&skv=2019-07-07&st=2021-11-04T02%3A53%3A32Z&se=2021-11-04T11%3A03%3A32Z&sp=r',\n  'logs/azureml/dataprep/backgroundProcess.log': 'https://wspai9150022834122947.blob.core.windows.net/azureml/ExperimentRun/dcid.train-heart_1635994987_e1effee5/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=yZa9SyVlhUgxfJcDPvPlQssjyTDMq0JXie6APj6%2BJ1w%3D&skoid=e5e6cfcb-9f6d-4652-b9cf-6f37b8f74f99&sktid=c609a0ec-a5e3-4631-9686-192280bd9151&skt=2021-11-04T02%3A53%3A10Z&ske=2021-11-05T11%3A03%3A10Z&sks=b&skv=2019-07-07&st=2021-11-04T02%3A53%3A32Z&se=2021-11-04T11%3A03%3A32Z&sp=r',\n  'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://wspai9150022834122947.blob.core.windows.net/azureml/ExperimentRun/dcid.train-heart_1635994987_e1effee5/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=zTBEfizfTFOEcNZoeioNEqV1HbR5ELnnfBkkc0aLKNs%3D&skoid=e5e6cfcb-9f6d-4652-b9cf-6f37b8f74f99&sktid=c609a0ec-a5e3-4631-9686-192280bd9151&skt=2021-11-04T02%3A53%3A10Z&ske=2021-11-05T11%3A03%3A10Z&sks=b&skv=2019-07-07&st=2021-11-04T02%3A53%3A32Z&se=2021-11-04T11%3A03%3A32Z&sp=r'},\n 'submittedBy': 'Carla Sena Santiago'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635995015795
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check metrics."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = run.get_metrics()\r\n",
        "for key in metrics.keys():\r\n",
        "    print(key, metrics.get(key))\r\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Accuracy 0.8289473684210527\nAUC 0.9270613107822411\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635995195055
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}